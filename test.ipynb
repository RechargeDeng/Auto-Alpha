{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "56ef40df",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def norm_cdf(x):\n",
        "    \"\"\"标准正态分布的累积分布函数 (CDF)\"\"\"\n",
        "    return 0.5 * (1 + math.erf(x / math.sqrt(2)))\n",
        "\n",
        "\n",
        "def black_scholes_call(S, K, T, r, sigma):\n",
        "    \"\"\"Black-Scholes 看涨期权价格\"\"\"\n",
        "    if T <= 0:\n",
        "        return max(S - K, 0)\n",
        "    d1 = (math.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * math.sqrt(T))\n",
        "    d2 = d1 - sigma * math.sqrt(T)\n",
        "    return S * norm_cdf(d1) - K * math.exp(-r * T) * norm_cdf(d2)\n",
        "\n",
        "\n",
        "def black_scholes_put(S, K, T, r, sigma):\n",
        "    \"\"\"Black-Scholes 看跌期权价格\"\"\"\n",
        "    if T <= 0:\n",
        "        return max(K - S, 0)\n",
        "    d1 = (math.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * math.sqrt(T))\n",
        "    d2 = d1 - sigma * math.sqrt(T)\n",
        "    return K * math.exp(-r * T) * norm_cdf(-d2) - S * norm_cdf(-d1)\n",
        "\n",
        "\n",
        "def straddle_price(spot, strike, iv, days_to_expiry, rate):\n",
        "    \"\"\"\n",
        "    计算 Straddle（跨式组合）价格 = 1 份看涨 + 1 份看跌（同一行权价）\n",
        "\n",
        "    参数:\n",
        "        spot: 现货价格 (S)\n",
        "        strike: 行权价格 (K)\n",
        "        iv: 隐含波动率，小数形式 (如 0.2 表示 20%)\n",
        "        days_to_expiry: 到期天数\n",
        "        rate: 无风险利率，年化小数形式 (如 0.05 表示 5%)\n",
        "\n",
        "    返回:\n",
        "        straddle 总价格\n",
        "    \"\"\"\n",
        "    T = days_to_expiry / 365.0  # 转换为年\n",
        "    call = black_scholes_call(spot, strike, T, rate, iv)\n",
        "    put = black_scholes_put(spot, strike, T, rate, iv)\n",
        "    return call + put"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5d24c95a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Straddle 价格: 9.3766\n"
          ]
        }
      ],
      "source": [
        "# 示例：现货 100，行权 100，隐含波动率 20%，30 天到期，利率 5%\n",
        "spot = 75\n",
        "strike = 75\n",
        "iv = 0.8015       # 20%\n",
        "days = 14\n",
        "rate = 0.0425        # 5%\n",
        "\n",
        "price = straddle_price(spot, strike, iv, days, rate)\n",
        "print(f\"Straddle 价格: {price:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9f5dd96b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Straddle 价格: 33.2810\n"
          ]
        }
      ],
      "source": [
        "# 示例：现货 100，行权 100，隐含波动率 20%，30 天到期，利率 5%\n",
        "spot = 445\n",
        "strike = 445\n",
        "iv = 0.4791       # 20%\n",
        "days = 14\n",
        "rate = 0.0425        # 5%\n",
        "\n",
        "price = straddle_price(spot, strike, iv, days, rate)\n",
        "print(f\"Straddle 价格: {price:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "65a17efb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "最终金额: 824.43 万美金\n"
          ]
        }
      ],
      "source": [
        "# 每月投入 0.5w 美金，年化收益率 70%，6年的收益率计算\n",
        "# 月收益率 = (1 + 0.7)^(1/12) - 1\n",
        "monthly_rate = (1.5) ** (1/12)\n",
        "monthly_investment = 0.5  # 万美金\n",
        "\n",
        "# 72个月的复利计算\n",
        "total = 0\n",
        "for month in range(120):  # 6年 = 72个月\n",
        "    total = total * monthly_rate + monthly_investment\n",
        "\n",
        "print(f\"最终金额: {total:.2f} 万美金\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "95d4a309",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "最终金额: 124.69 万美金\n"
          ]
        }
      ],
      "source": [
        "monthly_rate = (1.5) \n",
        "monthly_investment = 6  # 万美金\n",
        "\n",
        "# 72个月的复利计算\n",
        "total = 0\n",
        "for month in range(6):  # 6年 = 72个月\n",
        "    total = total * monthly_rate + monthly_investment\n",
        "\n",
        "print(f\"最终金额: {total:.2f} 万美金\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "32745274",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymupdf\n",
        "\n",
        "doc = pymupdf.open(\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/1601.00991v3.pdf\")\n",
        "text = \"\\n\".join(page.get_text() for page in doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ef7faea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 103 alpha chunks\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def split_alpha101(text: str):\n",
        "    \"\"\"\n",
        "    Split Alpha101 paper into chunks by Alpha# definitions.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"(Alpha#\\d+[\\s\\S]*?)(?=Alpha#\\d+|$)\")\n",
        "    matches = pattern.findall(text)\n",
        "\n",
        "    chunks = []\n",
        "    for m in matches:\n",
        "        header = re.search(r\"Alpha#(\\d+)\", m)\n",
        "        alpha_id = int(header.group(1)) if header else None\n",
        "\n",
        "        chunks.append({\n",
        "            \"alpha_id\": alpha_id,\n",
        "            \"content\": m.strip()\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "\n",
        "alpha_chunks = split_alpha101(text)\n",
        "print(f\"Extracted {len(alpha_chunks)} alpha chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "f0e417ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha_chunks = alpha_chunks[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "6ec02923",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_alpha101_documents(alpha_chunks):\n",
        "    docs = []\n",
        "\n",
        "    for c in alpha_chunks:\n",
        "        alpha_id = c[\"alpha_id\"]\n",
        "        content = c[\"content\"]\n",
        "\n",
        "        docs.append({\n",
        "            \"text\": content,\n",
        "            \"metadata\": {\n",
        "                \"kb_type\": \"paper\",\n",
        "                \"paper\": \"Alpha101\",\n",
        "                \"alpha_id\": alpha_id,\n",
        "                \"source\": \"Kakushadze 2015\",\n",
        "                \"semantic_hint\": f\"alpha {alpha_id} quantitative trading signal formula\"\n",
        "            }\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "\n",
        "paper_docs = build_alpha101_documents(alpha_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "00b55b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    lines = text.splitlines()\n",
        "    lines = [l.strip() for l in lines if len(l.strip()) > 0]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "for d in paper_docs:\n",
        "    d[\"text\"] = clean_text(d[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "5e6ae810",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "d741ee30",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x16bea2130>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x118faae50>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "5b52f396",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "texts = [d[\"text\"] for d in paper_docs]\n",
        "metadatas = [d[\"metadata\"] for d in paper_docs]\n",
        "\n",
        "paper_db = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    metadatas=metadatas,\n",
        "    embedding=embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "7c8916f5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x16c462dc0>"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "302bd290",
      "metadata": {},
      "outputs": [],
      "source": [
        "paper_db.save_local(\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_alpha101_paper_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "638a60cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved FAISS DB to /Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer1_modules, size=15\n",
            "Saved FAISS DB to /Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer2_fields, size=460\n",
            "Saved FAISS DB to /Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_operators, size=38\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x10a0f6ca0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "def load_jsonl(path):\n",
        "    records = []\n",
        "    bad_lines = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for idx, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "\n",
        "            # 跳过空行\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                records.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                bad_lines.append((idx, line, str(e)))\n",
        "\n",
        "    if bad_lines:\n",
        "        print(f\"[WARN] {len(bad_lines)} invalid json lines in {path}\")\n",
        "        for idx, line, err in bad_lines[:5]:\n",
        "            print(f\"Line {idx}: {err}\")\n",
        "            print(line[:200])\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    return records\n",
        "\n",
        "def build_embedding_text(item):\n",
        "    \"\"\"\n",
        "    Construct text used for embedding.\n",
        "    \"\"\"\n",
        "    parts = [\n",
        "        f\"Name: {item.get('name', '')}\",\n",
        "        f\"Description: {item.get('description', '')}\",\n",
        "        f\"Semantic: {item.get('semantic_text', '')}\",\n",
        "    ]\n",
        "    return \"\\n\".join(p for p in parts if p.strip())\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\"\n",
        ")\n",
        "\n",
        "def build_faiss_from_jsonl(jsonl_path, save_path):\n",
        "    records = load_jsonl(jsonl_path)\n",
        "\n",
        "    texts = []\n",
        "    metadatas = []\n",
        "\n",
        "    for item in records:\n",
        "        texts.append(build_embedding_text(item))\n",
        "        metadatas.append(item)  # 整条 metadata 保留，方便回溯\n",
        "\n",
        "    db = FAISS.from_texts(\n",
        "        texts=texts,\n",
        "        metadatas=metadatas,\n",
        "        embedding=embeddings\n",
        "    )\n",
        "\n",
        "    db.save_local(save_path)\n",
        "    print(f\"Saved FAISS DB to {save_path}, size={len(texts)}\")\n",
        "\n",
        "    return db\n",
        "\n",
        "build_faiss_from_jsonl(\n",
        "    jsonl_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/rag_fields/layer1_modules.jsonl\",\n",
        "    save_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer1_modules\"\n",
        ")\n",
        "\n",
        "build_faiss_from_jsonl(\n",
        "    jsonl_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/rag_fields/layer2_fields.jsonl\",\n",
        "    save_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer2_fields\"\n",
        ")\n",
        "\n",
        "build_faiss_from_jsonl(\n",
        "    jsonl_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/rag_fields/op.jsonl\",\n",
        "    save_path=\"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_operators\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a7b7f4e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "layer1_db = FAISS.load_local(\n",
        "    \"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer1_modules\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "layer2_db = FAISS.load_local(\n",
        "    \"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_layer2_fields\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "op_db = FAISS.load_local(\n",
        "    \"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_operators\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "papers_db = FAISS.load_local(\n",
        "    \"/Users/deng/Desktop/alpha-gpt/alpha-gpt/src/agent/database/faiss_alpha101_paper_db\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e5850bea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alpha ID: 69\n",
            "Alpha#69: ((rank(ts_max(delta(IndNeutralize(vwap, IndClass.industry), 2.72412), \n",
            "4.79344))^Ts_Rank(correlation(((close * 0.490655) + (vwap * (1 - 0.490655))), adv20, 4.92416), \n",
            "9.0615)) * -1)\n",
            "================================================================================\n",
            "Alpha ID: 58\n",
            "Alpha#58: (-1 * Ts_Rank(decay_linear(correlation(IndNeutralize(vwap, IndClass.sector), volume, \n",
            "3.92795), 7.89291), 5.50322))\n",
            "================================================================================\n",
            "Alpha ID: 10\n",
            "Alpha#10: rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) \n",
            "? delta(close, 1) : (-1 * delta(close, 1)))))\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "query = \"order cancellation information and short term price reversal\"\n",
        "\n",
        "docs = papers_db.similarity_search(query, k=3)\n",
        "\n",
        "for d in docs:\n",
        "    print(\"Alpha ID:\", d.metadata[\"alpha_id\"])\n",
        "    print(d.page_content[:300])\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c16550a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CancelLine.CancelLargeBuyDuration_ChangeRate\n",
            "Relative change rate of timing associated with large buy-side canceled orders compared to the previous 5-minute interval.\n",
            "----------------------------------------\n",
            "CancelLine.CancelLargeBuyVolume_ChangeRate\n",
            "Relative change rate of large buy-side canceled order volume compared to the previous 5-minute interval.\n",
            "----------------------------------------\n",
            "CancelLine_vs_TradeLine.LargeBuyVolume_Ratio\n",
            "Ratio of large buy-side volume between canceled orders and executed trades.\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query = \"change rate of large buy order cancellations\"\n",
        "\n",
        "docs = layer2_db.similarity_search(query, k=3)\n",
        "\n",
        "for d in docs:\n",
        "    print(d.metadata[\"name\"])\n",
        "    print(d.metadata[\"description\"])\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e9957a51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-4-0613\n",
            "gpt-4\n",
            "gpt-3.5-turbo\n",
            "gpt-5.2-codex\n",
            "gpt-4o-mini-tts-2025-12-15\n",
            "gpt-realtime-mini-2025-12-15\n",
            "gpt-audio-mini-2025-12-15\n",
            "chatgpt-image-latest\n",
            "davinci-002\n",
            "babbage-002\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "dall-e-3\n",
            "dall-e-2\n",
            "gpt-4-1106-preview\n",
            "gpt-3.5-turbo-1106\n",
            "tts-1-hd\n",
            "tts-1-1106\n",
            "tts-1-hd-1106\n",
            "text-embedding-3-small\n",
            "text-embedding-3-large\n",
            "gpt-4-0125-preview\n",
            "gpt-4-turbo-preview\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4-turbo\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4o\n",
            "gpt-4o-2024-05-13\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-mini\n",
            "gpt-4o-2024-08-06\n",
            "chatgpt-4o-latest\n",
            "gpt-4o-audio-preview\n",
            "gpt-4o-realtime-preview\n",
            "omni-moderation-latest\n",
            "omni-moderation-2024-09-26\n",
            "gpt-4o-realtime-preview-2024-12-17\n",
            "gpt-4o-audio-preview-2024-12-17\n",
            "gpt-4o-mini-realtime-preview-2024-12-17\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "o1-2024-12-17\n",
            "o1\n",
            "gpt-4o-mini-realtime-preview\n",
            "gpt-4o-mini-audio-preview\n",
            "o3-mini\n",
            "o3-mini-2025-01-31\n",
            "gpt-4o-2024-11-20\n",
            "gpt-4o-search-preview-2025-03-11\n",
            "gpt-4o-search-preview\n",
            "gpt-4o-mini-search-preview-2025-03-11\n",
            "gpt-4o-mini-search-preview\n",
            "gpt-4o-transcribe\n",
            "gpt-4o-mini-transcribe\n",
            "o1-pro-2025-03-19\n",
            "o1-pro\n",
            "gpt-4o-mini-tts\n",
            "o3-2025-04-16\n",
            "o4-mini-2025-04-16\n",
            "o3\n",
            "o4-mini\n",
            "gpt-4.1-2025-04-14\n",
            "gpt-4.1\n",
            "gpt-4.1-mini-2025-04-14\n",
            "gpt-4.1-mini\n",
            "gpt-4.1-nano-2025-04-14\n",
            "gpt-4.1-nano\n",
            "gpt-image-1\n",
            "gpt-4o-realtime-preview-2025-06-03\n",
            "gpt-4o-audio-preview-2025-06-03\n",
            "gpt-4o-transcribe-diarize\n",
            "gpt-5-chat-latest\n",
            "gpt-5-2025-08-07\n",
            "gpt-5\n",
            "gpt-5-mini-2025-08-07\n",
            "gpt-5-mini\n",
            "gpt-5-nano-2025-08-07\n",
            "gpt-5-nano\n",
            "gpt-audio-2025-08-28\n",
            "gpt-realtime\n",
            "gpt-realtime-2025-08-28\n",
            "gpt-audio\n",
            "gpt-5-codex\n",
            "gpt-image-1-mini\n",
            "gpt-5-pro-2025-10-06\n",
            "gpt-5-pro\n",
            "gpt-audio-mini\n",
            "gpt-audio-mini-2025-10-06\n",
            "gpt-5-search-api\n",
            "gpt-realtime-mini\n",
            "gpt-realtime-mini-2025-10-06\n",
            "sora-2\n",
            "sora-2-pro\n",
            "gpt-5-search-api-2025-10-14\n",
            "gpt-5.1-chat-latest\n",
            "gpt-5.1-2025-11-13\n",
            "gpt-5.1\n",
            "gpt-5.1-codex\n",
            "gpt-5.1-codex-mini\n",
            "gpt-5.1-codex-max\n",
            "gpt-image-1.5\n",
            "gpt-5.2-2025-12-11\n",
            "gpt-5.2\n",
            "gpt-5.2-pro-2025-12-11\n",
            "gpt-5.2-pro\n",
            "gpt-5.2-chat-latest\n",
            "gpt-4o-mini-transcribe-2025-12-15\n",
            "gpt-4o-mini-transcribe-2025-03-20\n",
            "gpt-4o-mini-tts-2025-03-20\n",
            "gpt-3.5-turbo-16k\n",
            "tts-1\n",
            "whisper-1\n",
            "text-embedding-ada-002\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "models = client.models.list()\n",
        "for m in models.data:\n",
        "    print(m.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a62563e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
